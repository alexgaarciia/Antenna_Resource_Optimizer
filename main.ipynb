{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b55994",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55254251",
   "metadata": {},
   "source": [
    "## Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05961ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "antennas = [\n",
    "    {'id': 'A1', 'x': 0, 'y': 0, 'bandwidth': 100},\n",
    "    {'id': 'A2', 'x': 100, 'y': 0, 'bandwidth': 80},\n",
    "    {'id': 'A3', 'x': 50, 'y': 86, 'bandwidth': 120}\n",
    "]\n",
    "users = [\n",
    "    {'id': 'U1', 'x': 10, 'y': 30},\n",
    "    {'id': 'U2', 'x': 60, 'y': 20},\n",
    "    {'id': 'U3', 'x': 100, 'y': 60},\n",
    "    {'id': 'U4', 'x': 20, 'y': 90}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7497c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_antennas = pd.DataFrame(antennas)\n",
    "df_antennas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57293cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.DataFrame(users)\n",
    "df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c68304",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(df_antennas[\"x\"], df_antennas[\"y\"], 'ro', label='Antennas', markersize=10)\n",
    "plt.plot(df_users[\"x\"], df_users[\"y\"], 'bs', label='Users', markersize=8)\n",
    "\n",
    "for i, row in df_antennas.iterrows():\n",
    "    plt.text(row[\"x\"] + 2, row[\"y\"] + 2, row[\"id\"], color='red', fontsize=10)\n",
    "for i, row in df_users.iterrows():\n",
    "    plt.text(row[\"x\"] + 2, row[\"y\"] + 2, row[\"id\"], color='blue', fontsize=10)\n",
    "    \n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Antennas and Users Positions')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.zeros((len(users), len(antennas)))\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    for j, antenna in enumerate(antennas):\n",
    "        distance_matrix[i][j] = np.linalg.norm([user['x'] - antenna['x'], user['y'] - antenna['y']])\n",
    "\n",
    "user_ids = [user['id'] for user in users]\n",
    "antenna_ids = [antenna['id'] for antenna in antennas]\n",
    "df_distance = pd.DataFrame(distance_matrix, index=user_ids, columns=antenna_ids)\n",
    "df_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4251a",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db11319",
   "metadata": {},
   "source": [
    "OpenAI's `gym` is a Python library used to build training environments for RL agents. It doesn't train the agent itself, but rather creates the \"world\" in which the agent operates. What we define is:\n",
    "\n",
    "- What the agent can observe.\n",
    "- What actions the agent can take.\n",
    "- What reward the agent receives for those actions.\n",
    "- When the episode ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2c718",
   "metadata": {},
   "source": [
    "A `gym` environment always has 5 functions:\n",
    "- `__init__()`: Initializes the environment (users, antennas, etc.).\n",
    "- `reset()`: Starts a new episode, returns the initial state.\n",
    "- `step(action)`: Applies an action, returns: (new state, reward, done, info).\n",
    "- `render()`: (Optional) Visually shows what's happening.\n",
    "- `close()`: (Optional) Frees resources at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0012705",
   "metadata": {},
   "source": [
    "### Simple Case: Assignment of users to a unique antenna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAntennaEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(SimpleAntennaEnv, self).__init__()\n",
    "\n",
    "        self.antennas = [\n",
    "            {'id': 'A0', 'x': 10, 'y': 90, 'bandwidth': 2},\n",
    "            {'id': 'A1', 'x': 90, 'y': 90, 'bandwidth': 2}\n",
    "        ]\n",
    "\n",
    "        self.num_antennas = len(self.antennas)\n",
    "        self.num_users = 3\n",
    "\n",
    "        # The observation space is what the agent sees: in this case the distances of each user to each antenna and the available bandwidth of each antenna\n",
    "        # spaces.Box represents a continuous space (the observation is a vector of real numbers)\n",
    "        # Example of observation: [distance_to_A0, distance_to_A1, remaining_capacity_A0, remaining_capacity_A1]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_antennas*2,), dtype=np.float32)\n",
    "        \n",
    "        # The action space is what the agent can do: in this case, select one of the antennas to connect to\n",
    "        # spaces.Discrete means the agent must pick an integer action from a finite set of actions (0 to num_antennas-1)\n",
    "        self.action_space = spaces.Discrete(self.num_antennas)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_user = 0\n",
    "        self.bandwidth_used = np.zeros(self.num_antennas)\n",
    "\n",
    "        # Random users per episode to guarantee variability\n",
    "        self.users = []\n",
    "        for i in range(self.num_users):\n",
    "            self.users.append({\n",
    "                'id': f'U{i}',\n",
    "                'x': np.random.uniform(0, 100),\n",
    "                'y': np.random.uniform(0, 100)\n",
    "            })\n",
    "\n",
    "        self.distances = self.compute_distances()\n",
    "        self.connections = []\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def compute_distances(self):\n",
    "        dist_matrix = np.zeros((self.num_users, self.num_antennas))\n",
    "        for i, user in enumerate(self.users):\n",
    "            for j, antenna in enumerate(self.antennas):\n",
    "                dist_matrix[i][j] = np.linalg.norm(\n",
    "                    [user['x'] - antenna['x'], user['y'] - antenna['y']]\n",
    "                )\n",
    "        return dist_matrix\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Method that returns the state (observation) for the current user, which will be seen\n",
    "        by the agent during training/inference.\n",
    "        \"\"\"\n",
    "        distances = self.distances[self.current_user]\n",
    "        capacities = np.array([\n",
    "            self.antennas[i]['bandwidth'] - self.bandwidth_used[i]\n",
    "            for i in range(self.num_antennas)\n",
    "        ])\n",
    "        return np.concatenate([distances, capacities]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        user_idx = self.current_user\n",
    "        antenna_idx = action\n",
    "        distance = self.distances[user_idx][antenna_idx]\n",
    "\n",
    "        if distance <= 50:\n",
    "            if self.bandwidth_used[antenna_idx] < self.antennas[antenna_idx]['bandwidth']:\n",
    "                reward = 1\n",
    "                self.bandwidth_used[antenna_idx] += 1\n",
    "            else:\n",
    "                reward = 0  \n",
    "        else:\n",
    "            reward = -1 \n",
    "\n",
    "        self.connections.append((user_idx, antenna_idx))\n",
    "        self.current_user += 1\n",
    "        terminated = self.current_user >= self.num_users\n",
    "        truncated = False\n",
    "\n",
    "        if terminated:\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        else:\n",
    "            obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        for antenna in self.antennas:\n",
    "            plt.plot(antenna['x'], antenna['y'], 'ro', markersize=10)\n",
    "            plt.text(antenna['x'] + 1, antenna['y'] + 1, antenna['id'], color='red')\n",
    "\n",
    "        for user in self.users:\n",
    "            plt.plot(user['x'], user['y'], 'bs', markersize=8)\n",
    "            plt.text(user['x'] + 1, user['y'] + 1, user['id'], color='blue')\n",
    "\n",
    "        for user_idx, antenna_idx in self.connections:\n",
    "            user = self.users[user_idx]\n",
    "            antenna = self.antennas[antenna_idx]\n",
    "            plt.plot(\n",
    "                [user['x'], antenna['x']],\n",
    "                [user['y'], antenna['y']],\n",
    "                'g--'\n",
    "            )\n",
    "\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Antennas, Users and Connections')\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb759c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the environment is properly implemented\n",
    "check_env(SimpleAntennaEnv())\n",
    "\n",
    "# Create the environment\n",
    "env = SimpleAntennaEnv()\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "env.render()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Agent total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb6867",
   "metadata": {},
   "source": [
    "### Complex Case: Assignment of users to several antennas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8ec3b",
   "metadata": {},
   "source": [
    "`self.observation_space`: It is what the agent sees, which in this case is the distances of each user to each antenna and the available bandwith of each antenna. With this the agent decides what action to take. In our case:\n",
    "```\n",
    "self.observation_space = spaces.Box(\n",
    "    low=0.0,\n",
    "    high=np.inf,\n",
    "    shape=(self.num_antennas * 2,),\n",
    "    dtype=np.float32\n",
    ")\n",
    "```\n",
    "\n",
    "**spaces.Box** represents a continuous space (the observation is a vector of real numbers). We are telling the agent that it will receive a 2xnum_antennas vector:\n",
    "```\n",
    "[\n",
    " dist_a0, dist_a1, dist_a2, dist_a3, dist_a4,\n",
    " cap_a0,  cap_a1,  cap_a2,  cap_a3,  cap_a4\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8b2f9",
   "metadata": {},
   "source": [
    "`self.action_space`: It is what the agent can do, which in this case is select one of the antennas to connect to.\n",
    "```\n",
    "self.action_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(self.num_antennas,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "```\n",
    "\n",
    "**spaces.Discrete** the agent must pick an integer action from a finite set of actions (0 to num_antennas-1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAntennaEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the environment with:\n",
    "        - 5 fixed antennas, each with 1000 Mbps total capacity\n",
    "        - A fixed number of users (default: 3)\n",
    "        - Observation and action space definitions\n",
    "        \"\"\"\n",
    "        super(MultiAntennaEnv, self).__init__()\n",
    "\n",
    "        # Fixed antennas, each with 1000 Mbps total capacity\n",
    "        self.antennas = [\n",
    "            {'id': 'A0', 'x': 10,  'y': 90, 'bandwidth': 1000.0},\n",
    "            {'id': 'A1', 'x': 90,  'y': 90, 'bandwidth': 1000.0},\n",
    "            {'id': 'A2', 'x': 50,  'y': 50, 'bandwidth': 1000.0},\n",
    "            {'id': 'A3', 'x': 20,  'y': 20, 'bandwidth': 1000.0},\n",
    "            {'id': 'A4', 'x': 80,  'y': 20, 'bandwidth': 1000.0},\n",
    "        ]\n",
    "\n",
    "        self.num_antennas = len(self.antennas)\n",
    "        self.num_users = 3\n",
    "        self.max_distance = 50.0\n",
    "        self.user_demand = 50.0  # Mbps required per user\n",
    "\n",
    "        # Observation: distances to antennas + remaining capacity per antenna\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=np.inf,\n",
    "            shape=(self.num_antennas * 2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Action: fraction of the user's demand assigned to each antenna\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0.0, high=1.0,\n",
    "            shape=(self.num_antennas,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment at the beginning of an episode:\n",
    "        - Set bandwidth usage to zero\n",
    "        - Generate new random user positions\n",
    "        - Precompute distances\n",
    "        - Return the initial observation\n",
    "        \"\"\"\n",
    "        self.current_user = 0\n",
    "        self.bandwidth_used = np.zeros(self.num_antennas)\n",
    "\n",
    "        # Generate random users within a 100x100 area\n",
    "        self.users = []\n",
    "        for i in range(self.num_users):\n",
    "            self.users.append({\n",
    "                'id': f'U{i}',\n",
    "                'x': np.random.uniform(0, 100),\n",
    "                'y': np.random.uniform(0, 100)\n",
    "            })\n",
    "\n",
    "        self.distances = self.compute_distances()\n",
    "        self.connections = []\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def compute_distances(self):\n",
    "        \"\"\"\n",
    "        Compute and return a matrix of Euclidean distances between\n",
    "        each user and each antenna (shape: num_users × num_antennas).\n",
    "        \"\"\"\n",
    "        dist_matrix = np.zeros((self.num_users, self.num_antennas))\n",
    "        for i, user in enumerate(self.users):\n",
    "            for j, antenna in enumerate(self.antennas):\n",
    "                dist_matrix[i][j] = np.linalg.norm(\n",
    "                    [user['x'] - antenna['x'], user['y'] - antenna['y']]\n",
    "                )\n",
    "        return dist_matrix\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Return the current observation:\n",
    "        - Distances from current user to all antennas\n",
    "        - Remaining bandwidth of each antenna\n",
    "        \"\"\"\n",
    "        distances = self.distances[self.current_user]\n",
    "        capacities = np.array([\n",
    "            self.antennas[i]['bandwidth'] - self.bandwidth_used[i]\n",
    "            for i in range(self.num_antennas)\n",
    "        ])\n",
    "        return np.concatenate([distances, capacities]).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment:\n",
    "        - The agent proposes a fractional assignment to each antenna\n",
    "        - We compute how much bandwidth is actually assigned (respecting distance and capacity limits)\n",
    "        - Reward is based on whether the user's demand is fully, partially, or not met\n",
    "        - Move to the next user\n",
    "        \"\"\"\n",
    "        action = np.clip(action, 0, 1)  # Ensure action is within bounds\n",
    "\n",
    "        # Normalize if sum of fractions exceeds 1.0\n",
    "        total_fraction = np.sum(action)\n",
    "        if total_fraction > 1.0:\n",
    "            action = action / total_fraction\n",
    "           \n",
    "        total_assigned = 0.0  # keeps track of how many total Mbps were assigned for the user\n",
    "        valid_assignments = np.zeros(self.num_antennas)  # keeps track of how many Mbps were actually assigned per antenna\n",
    "\n",
    "        for j in range(self.num_antennas):\n",
    "            # Distance between current user and antenna j\n",
    "            dist = self.distances[self.current_user][j]\n",
    "\n",
    "            # Number of Mbps the agent wants to assign to antenna j\n",
    "            assign_mbps = action[j] * self.user_demand\n",
    "\n",
    "            # Check if the antenna is whithin range and has enough bandwidth\n",
    "            if dist <= self.max_distance:\n",
    "                if self.bandwidth_used[j] + assign_mbps <= self.antennas[j]['bandwidth']:\n",
    "                    valid_assignments[j] = assign_mbps\n",
    "                    self.bandwidth_used[j] += assign_mbps\n",
    "                    total_assigned += assign_mbps\n",
    "\n",
    "        # Store which antennas were assigned to the current  user and with how many Mbps\n",
    "        self.connections.append((self.current_user, valid_assignments.copy()))\n",
    "\n",
    "        if total_assigned >= self.user_demand:\n",
    "            reward = 1.0\n",
    "        elif total_assigned > 0:\n",
    "            reward = 0.5\n",
    "        else:\n",
    "            reward = -1.0\n",
    "\n",
    "        self.current_user += 1\n",
    "        terminated = self.current_user >= self.num_users\n",
    "        truncated = False\n",
    "\n",
    "        if terminated:\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        else:\n",
    "            obs = self._get_observation()\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Display a 2D plot showing:\n",
    "        - Red circles: antennas with usage/remaining bandwidth\n",
    "        - Blue squares: users with total Mbps received\n",
    "        - Green dashed lines: links representing partial connections\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "        for i, antenna in enumerate(self.antennas):\n",
    "            used = self.bandwidth_used[i]\n",
    "            total = antenna['bandwidth']\n",
    "            remaining = total - used\n",
    "\n",
    "            ax.plot(antenna['x'], antenna['y'], 'ro', markersize=10)\n",
    "            ax.text(\n",
    "                antenna['x'] + 1,\n",
    "                antenna['y'] + 1,\n",
    "                f\"{antenna['id']}\\nused: {used:.0f} / left: {remaining:.0f} Mbps\",\n",
    "                color='red',\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "        for user_idx, user in enumerate(self.users):\n",
    "            ax.plot(user['x'], user['y'], 'bs', markersize=8)\n",
    "\n",
    "            # Sum of assigned Mbps from all antennas to this user\n",
    "            if user_idx < len(self.connections):\n",
    "                assignment = self.connections[user_idx][1]\n",
    "                received = np.sum(assignment)\n",
    "                label = f\"{user['id']}\\nreceived: {received:.1f} Mbps\"\n",
    "            else:\n",
    "                label = f\"{user['id']}\"\n",
    "\n",
    "            ax.text(user['x'] + 1, user['y'] + 1, label, color='blue', fontsize=9)\n",
    "\n",
    "        for user_idx, assignment in self.connections:\n",
    "            user = self.users[user_idx]\n",
    "            for j in range(self.num_antennas):\n",
    "                if assignment[j] > 0:\n",
    "                    antenna = self.antennas[j]\n",
    "                    ax.plot([user['x'], antenna['x']], [user['y'], antenna['y']], 'g--')\n",
    "\n",
    "        ax.set_xlabel('X coordinate')\n",
    "        ax.set_ylabel('Y coordinate')\n",
    "        ax.set_title('User connections and antenna usage (in Mbps)')\n",
    "        ax.grid(True)\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = MultiAntennaEnv()\n",
    "check_env(env)\n",
    "\n",
    "# Create the PPO model\n",
    "# PPO = Proximal Policy Optimization algorithm, a popular reinforcement learning algorithm that learns using clipped policy gradients\n",
    "# A policy is the neural network that the agent uses to decide which action to take given an observation\n",
    "# A policy gradient is a way to improve the policy: \"If the action gave a high reward, make it more likely next time\"\n",
    "\n",
    "# Problem! -> Older algorithms' update could be too agressive: \n",
    "#   - If an action got a very high reward, the model might overreact\n",
    "#   - It could jump to a new policy that’s too different from the previous one\n",
    "#   - That breaks learning\n",
    "\n",
    "# PPO's Solution! -> Clipped policy gradients: Only allow the policy to change a small amount per udpate\n",
    "\n",
    "# Then, we need to choose a policy architecture, which in this case is a simple MLP\n",
    "# PPO uses the MLP to learn the policy, and it clips the gradients during the learing step\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd36041",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "env.render()\n",
    "print(f\"\\nTotal reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
